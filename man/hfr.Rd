% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hfr.R
\name{hfr}
\alias{hfr}
\title{Fit a hierarchical feature regression}
\usage{
hfr(x, y, nu = 1, q = NULL, intercept = TRUE, standardize = TRUE, ...)
}
\arguments{
\item{x}{Input matrix, of dimension \eqn{(N\times p)}{(N x p)}; each row is an observation vector.}

\item{y}{Response variable.}

\item{nu}{The target effective degrees of freedom of the regression as a percentage of nvars.}

\item{q}{The quantile cut-off (in terms of information contributed) above which to consider levels in the hierarchy.}

\item{intercept}{Should intercept be fitted (default=TRUE).}

\item{standardize}{Logical flag for x variable standardization prior to fitting the model. The coefficients are always returned on the original scale. Default is \code{standardize=TRUE}.}

\item{...}{Additional arguments passed to \code{hclust}.}
}
\value{
An 'hfr' regression object.
}
\description{
HFR is a regularized regression estimator that decomposes a least squares
regression along a semi-supervised hierarchical graph, and shrinks coefficients
along the branches of the tree. The algorithm leads to group shrinkage in the
regression parameters and a reduction in the effective model degrees of freedom.
}
\details{
Shrinkage can be imposed by targeting an explicit effective degrees of freedom.
Setting the argument \code{nu} to a value between 0 and 1 controls the effective degrees of
freedom of the fitted object as a percentage of \eqn{p}{p}. When \eqn{p > N}{p > N}
'nu' is a percentage of \eqn{(N - 2)}{(N - 2)}.
If no \code{nu} is set, a linear regression with \code{nu = 1} is
estimated.

Hierarchical clustering is performed using \code{hclust}. Default is complete-linkage agglomerative nesting.

For high-dimensional problems, the hierarchy becomes very large. Setting \code{q} to a value below 1
reduces the number of levels used in the hierarchy. \code{q} represents a quantile-cutoff of amount of
information contributed by the levels. The default (\code{q = 1}) considers all levels.
}
\examples{
x = matrix(rnorm(100 * 20), 100, 20)
y = rnorm(100)
fit = hfr(x, y, nu = 0.5)
coef(fit)

}
\references{
Pfitzinger, J. (2021).
Cluster Regularization via a Hierarchical Feature Regression.
arXiv 2107.04831[statML]
}
\seealso{
\code{cv.hfr}, \code{coef}, \code{plot} and \code{predict} methods
}
\author{
Johann Pfitzinger
}
